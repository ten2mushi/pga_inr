"""
HyperNetwork layers for generative PGA-INR.

HyperNetworks generate the weights for another network (the target network)
based on a conditioning signal (latent code). This enables learning a
continuous manifold of shapes parameterized by latent codes.

Architecture:
- HyperLayer: A functional layer that applies externally-generated weights
- HyperNetwork: Generates weights for all layers of the target network
"""

from typing import List, Tuple, Optional, Callable
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class HyperLayer(nn.Module):
    """
    A functional layer that applies externally-generated weights.

    Unlike nn.Linear which has fixed learned weights, HyperLayer receives
    weights as input during the forward pass. This allows the weights to
    be dynamically generated by a HyperNetwork.

    Supports batched weights: each item in the batch can have different weights.
    """

    def __init__(self, activation: Optional[Callable] = None):
        """
        Args:
            activation: Activation function to apply after linear transform.
                       Common choices: torch.sin, F.relu, None
        """
        super().__init__()
        self.activation = activation

    def forward(
        self,
        x: torch.Tensor,
        weights: torch.Tensor,
        bias: torch.Tensor
    ) -> torch.Tensor:
        """
        Apply the functional linear layer with given weights.

        Args:
            x: Input tensor of shape (B, N, in_dim)
            weights: Weight matrix of shape (B, out_dim, in_dim)
            bias: Bias vector of shape (B, out_dim)

        Returns:
            Output tensor of shape (B, N, out_dim)
        """
        # Batch matrix multiplication
        # x: (B, N, in_dim) -> transpose to (B, in_dim, N)
        # weights: (B, out_dim, in_dim)
        # result: (B, out_dim, N) -> transpose to (B, N, out_dim)
        out = torch.bmm(weights, x.transpose(1, 2)).transpose(1, 2)

        # Add bias (broadcast over N dimension)
        out = out + bias.unsqueeze(1)

        # Apply activation
        if self.activation is not None:
            out = self.activation(out)

        return out


class HyperNetwork(nn.Module):
    """
    Generates weights for a target network from a latent code.

    Maps: z → (W₁, b₁, W₂, b₂, ..., Wₙ, bₙ)

    The HyperNetwork is a small MLP that takes a latent code and outputs
    all the weights and biases needed for the target network.
    """

    def __init__(
        self,
        latent_dim: int,
        target_shapes: List[Tuple[int, int]],
        hidden_dim: int = 256,
        num_hidden: int = 2
    ):
        """
        Args:
            latent_dim: Dimension of the input latent code
            target_shapes: List of (in_dim, out_dim) for each layer of target network
            hidden_dim: Hidden dimension of the HyperNetwork MLP
            num_hidden: Number of hidden layers in HyperNetwork
        """
        super().__init__()

        self.latent_dim = latent_dim
        self.target_shapes = target_shapes
        self.num_target_layers = len(target_shapes)

        # Build the processor MLP
        layers = []
        in_dim = latent_dim

        for _ in range(num_hidden):
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.ReLU())
            in_dim = hidden_dim

        self.processor = nn.Sequential(*layers)

        # Create weight and bias generators for each target layer
        self.weight_generators = nn.ModuleList()
        self.bias_generators = nn.ModuleList()

        for fan_in, fan_out in target_shapes:
            # Weight generator outputs flattened weight matrix
            self.weight_generators.append(nn.Linear(hidden_dim, fan_in * fan_out))
            # Bias generator
            self.bias_generators.append(nn.Linear(hidden_dim, fan_out))

        # Initialize generators with small weights for stability
        self._init_generators()

    def _init_generators(self):
        """Initialize weight generators for stable training."""
        for gen in self.weight_generators:
            nn.init.normal_(gen.weight, std=0.01)
            nn.init.zeros_(gen.bias)

        for gen in self.bias_generators:
            nn.init.normal_(gen.weight, std=0.01)
            nn.init.zeros_(gen.bias)

    def forward(
        self,
        z: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        Generate weights for the target network.

        Args:
            z: Latent code of shape (B, latent_dim)

        Returns:
            weights: List of weight tensors, each of shape (B, out_dim, in_dim)
            biases: List of bias tensors, each of shape (B, out_dim)
        """
        # Process latent code
        features = self.processor(z)

        # Generate weights and biases for each layer
        weights = []
        biases = []

        for i, (fan_in, fan_out) in enumerate(self.target_shapes):
            # Generate flattened weights and reshape
            W_flat = self.weight_generators[i](features)
            W = W_flat.view(-1, fan_out, fan_in)

            # Generate biases
            b = self.bias_generators[i](features)

            weights.append(W)
            biases.append(b)

        return weights, biases


class SirenHyperNetwork(HyperNetwork):
    """
    HyperNetwork specialized for SIREN target networks.

    Applies SIREN-style initialization scaling to generated weights.
    """

    def __init__(
        self,
        latent_dim: int,
        target_shapes: List[Tuple[int, int]],
        hidden_dim: int = 256,
        omega_0: float = 30.0
    ):
        super().__init__(latent_dim, target_shapes, hidden_dim)
        self.omega_0 = omega_0

    def forward(
        self,
        z: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """Generate SIREN-scaled weights."""
        weights, biases = super().forward(z)

        # Scale weights according to SIREN initialization
        scaled_weights = []
        for i, (W, (fan_in, fan_out)) in enumerate(zip(weights, self.target_shapes)):
            if i == 0:
                # First layer: scale to [-1/n, 1/n]
                scale = 1.0 / fan_in
            else:
                # Hidden layers: scale to [-√(6/n)/ω₀, √(6/n)/ω₀]
                scale = np.sqrt(6.0 / fan_in) / self.omega_0

            # Apply scaling (assuming generated weights are roughly unit variance)
            scaled_weights.append(W * scale)

        return scaled_weights, biases


class FunctionalSirenMLP(nn.Module):
    """
    A SIREN network that uses HyperLayer for all layers.

    This is the target network whose weights are generated by a HyperNetwork.
    """

    def __init__(
        self,
        layer_shapes: List[Tuple[int, int]],
        omega_0: float = 30.0
    ):
        """
        Args:
            layer_shapes: List of (in_dim, out_dim) for each layer
            omega_0: SIREN frequency parameter
        """
        super().__init__()

        self.omega_0 = omega_0
        self.num_layers = len(layer_shapes)

        # Create functional layers
        self.layers = nn.ModuleList()
        for i in range(len(layer_shapes) - 1):
            # Hidden layers use sine activation
            self.layers.append(HyperLayer(activation=torch.sin))

        # Final layer has no activation (or custom)
        self.layers.append(HyperLayer(activation=None))

    def forward(
        self,
        x: torch.Tensor,
        weights: List[torch.Tensor],
        biases: List[torch.Tensor]
    ) -> torch.Tensor:
        """
        Forward pass with provided weights.

        Args:
            x: Input of shape (B, N, in_dim)
            weights: List of weight tensors from HyperNetwork
            biases: List of bias tensors from HyperNetwork

        Returns:
            Output of shape (B, N, out_dim)
        """
        for i, layer in enumerate(self.layers):
            # Apply omega_0 scaling for SIREN
            if i < len(self.layers) - 1:
                # Hidden layers: scale input to linear layer
                x = layer(x, self.omega_0 * weights[i], self.omega_0 * biases[i])
            else:
                # Final layer: no omega scaling
                x = layer(x, weights[i], biases[i])

        return x


class ConditionalBatchNorm(nn.Module):
    """
    Conditional Batch Normalization for feature modulation.

    Alternative to full HyperNetwork: only modulates scale and shift
    of normalized features based on conditioning.
    """

    def __init__(
        self,
        num_features: int,
        condition_dim: int
    ):
        """
        Args:
            num_features: Number of features to normalize
            condition_dim: Dimension of conditioning vector
        """
        super().__init__()

        self.num_features = num_features

        # Batch normalization (without learned affine)
        self.bn = nn.BatchNorm1d(num_features, affine=False)

        # Condition-dependent scale and shift
        self.gamma_net = nn.Linear(condition_dim, num_features)
        self.beta_net = nn.Linear(condition_dim, num_features)

        # Initialize to identity transform
        nn.init.ones_(self.gamma_net.bias)
        nn.init.zeros_(self.gamma_net.weight)
        nn.init.zeros_(self.beta_net.bias)
        nn.init.zeros_(self.beta_net.weight)

    def forward(
        self,
        x: torch.Tensor,
        condition: torch.Tensor
    ) -> torch.Tensor:
        """
        Apply conditional batch normalization.

        Args:
            x: Features of shape (B, N, C) or (B, C)
            condition: Conditioning vector of shape (B, condition_dim)

        Returns:
            Normalized and modulated features
        """
        # Handle 3D input (B, N, C)
        if x.dim() == 3:
            B, N, C = x.shape
            x_flat = x.transpose(1, 2).reshape(B * N, C)
            x_norm = self.bn(x_flat).reshape(B, C, N).transpose(1, 2)
        else:
            x_norm = self.bn(x)

        # Generate scale and shift from condition
        gamma = self.gamma_net(condition)
        beta = self.beta_net(condition)

        # Apply modulation
        if x.dim() == 3:
            gamma = gamma.unsqueeze(1)
            beta = beta.unsqueeze(1)

        return gamma * x_norm + beta


class WeightNormalizedHyperNetwork(HyperNetwork):
    """
    HyperNetwork with weight normalization for improved stability.

    Separates weight magnitude from direction for more stable training.
    """

    def __init__(
        self,
        latent_dim: int,
        target_shapes: List[Tuple[int, int]],
        hidden_dim: int = 256
    ):
        super().__init__(latent_dim, target_shapes, hidden_dim)

        # Add magnitude predictors (one scalar magnitude per output neuron)
        self.magnitude_generators = nn.ModuleList()
        for fan_in, fan_out in target_shapes:
            self.magnitude_generators.append(nn.Linear(hidden_dim, fan_out))

    def forward(
        self,
        z: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """Generate weight-normalized weights."""
        features = self.processor(z)

        weights = []
        biases = []

        for i, (fan_in, fan_out) in enumerate(self.target_shapes):
            # Generate direction (unit norm)
            W_flat = self.weight_generators[i](features)
            W = W_flat.view(-1, fan_out, fan_in)

            # Normalize each row (output neuron)
            W_norm = F.normalize(W, dim=-1)

            # Generate magnitude
            g = F.softplus(self.magnitude_generators[i](features))
            g = g.unsqueeze(-1)

            # Combine
            W = g * W_norm

            # Bias
            b = self.bias_generators[i](features)

            weights.append(W)
            biases.append(b)

        return weights, biases
